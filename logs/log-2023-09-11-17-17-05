09/11/2023 17:17:05 - INFO - pytorch_pretrained_zen.modeling -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
09/11/2023 17:17:05 - INFO - __main__ -   {'do_train': True, 'do_test': False, 'do_predict': False, 'do_predict_uc': False, 'train_data_path': './data/mydata/larger/train.tsv', 'self_train_data_path': None, 'eval_data_path': './data/mydata/larger/test.tsv', 'input_file': None, 'output_file': None, 'output_file_tsv': None, 'use_bert': True, 'use_zen': False, 'bert_model': '/opt/Projects/Python/WMSeg/models/Zen', 'eval_model': None, 'cache_dir': '', 'max_seq_length': 300, 'max_ngram_size': 300, 'do_lower_case': False, 'train_batch_size': 16, 'eval_batch_size': 16, 'lambda_rate': 0.5, 'learning_rate': 1e-05, 'num_train_epochs': 50.0, 'warmup_proportion': 0.1, 'no_cuda': False, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'fp16': False, 'loss_scale': 0, 'server_ip': '', 'server_port': '', 'patient': 10, 'ngram_num_threshold': 2, 'av_threshold': 3, 'max_ngram_length': 5, 'model_name': 'geo_Bert_memory_sf', 'use_memory': True, 'use_gcn': False, 'decoder': 'softmax', 'ngram_flag': 'av', 'save_top': 1}
09/11/2023 17:17:05 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
09/11/2023 17:17:05 - INFO - __main__ -   # of word in train: 642: 
09/11/2023 17:17:05 - INFO - __main__ -   # of n-gram in memory: 1110
09/11/2023 17:17:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /opt/Projects/Python/WMSeg/models/Zen/vocab.txt
09/11/2023 17:17:05 - INFO - pytorch_pretrained_bert.modeling -   loading weights file /opt/Projects/Python/WMSeg/models/Zen/pytorch_model.bin
09/11/2023 17:17:05 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file /opt/Projects/Python/WMSeg/models/Zen/config.json
09/11/2023 17:17:05 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_hidden_word_layers": 6,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128,
  "word_size": 104089
}

09/11/2023 17:17:06 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertModel: ['bert.word_embeddings.word_embeddings.weight', 'bert.word_embeddings.token_type_embeddings.weight', 'bert.word_embeddings.LayerNorm.weight', 'bert.word_embeddings.LayerNorm.bias', 'bert.encoder.word_layers.0.attention.self.query.weight', 'bert.encoder.word_layers.0.attention.self.query.bias', 'bert.encoder.word_layers.0.attention.self.key.weight', 'bert.encoder.word_layers.0.attention.self.key.bias', 'bert.encoder.word_layers.0.attention.self.value.weight', 'bert.encoder.word_layers.0.attention.self.value.bias', 'bert.encoder.word_layers.0.attention.output.dense.weight', 'bert.encoder.word_layers.0.attention.output.dense.bias', 'bert.encoder.word_layers.0.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.0.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.0.intermediate.dense.weight', 'bert.encoder.word_layers.0.intermediate.dense.bias', 'bert.encoder.word_layers.0.output.dense.weight', 'bert.encoder.word_layers.0.output.dense.bias', 'bert.encoder.word_layers.0.output.LayerNorm.weight', 'bert.encoder.word_layers.0.output.LayerNorm.bias', 'bert.encoder.word_layers.1.attention.self.query.weight', 'bert.encoder.word_layers.1.attention.self.query.bias', 'bert.encoder.word_layers.1.attention.self.key.weight', 'bert.encoder.word_layers.1.attention.self.key.bias', 'bert.encoder.word_layers.1.attention.self.value.weight', 'bert.encoder.word_layers.1.attention.self.value.bias', 'bert.encoder.word_layers.1.attention.output.dense.weight', 'bert.encoder.word_layers.1.attention.output.dense.bias', 'bert.encoder.word_layers.1.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.1.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.1.intermediate.dense.weight', 'bert.encoder.word_layers.1.intermediate.dense.bias', 'bert.encoder.word_layers.1.output.dense.weight', 'bert.encoder.word_layers.1.output.dense.bias', 'bert.encoder.word_layers.1.output.LayerNorm.weight', 'bert.encoder.word_layers.1.output.LayerNorm.bias', 'bert.encoder.word_layers.2.attention.self.query.weight', 'bert.encoder.word_layers.2.attention.self.query.bias', 'bert.encoder.word_layers.2.attention.self.key.weight', 'bert.encoder.word_layers.2.attention.self.key.bias', 'bert.encoder.word_layers.2.attention.self.value.weight', 'bert.encoder.word_layers.2.attention.self.value.bias', 'bert.encoder.word_layers.2.attention.output.dense.weight', 'bert.encoder.word_layers.2.attention.output.dense.bias', 'bert.encoder.word_layers.2.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.2.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.2.intermediate.dense.weight', 'bert.encoder.word_layers.2.intermediate.dense.bias', 'bert.encoder.word_layers.2.output.dense.weight', 'bert.encoder.word_layers.2.output.dense.bias', 'bert.encoder.word_layers.2.output.LayerNorm.weight', 'bert.encoder.word_layers.2.output.LayerNorm.bias', 'bert.encoder.word_layers.3.attention.self.query.weight', 'bert.encoder.word_layers.3.attention.self.query.bias', 'bert.encoder.word_layers.3.attention.self.key.weight', 'bert.encoder.word_layers.3.attention.self.key.bias', 'bert.encoder.word_layers.3.attention.self.value.weight', 'bert.encoder.word_layers.3.attention.self.value.bias', 'bert.encoder.word_layers.3.attention.output.dense.weight', 'bert.encoder.word_layers.3.attention.output.dense.bias', 'bert.encoder.word_layers.3.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.3.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.3.intermediate.dense.weight', 'bert.encoder.word_layers.3.intermediate.dense.bias', 'bert.encoder.word_layers.3.output.dense.weight', 'bert.encoder.word_layers.3.output.dense.bias', 'bert.encoder.word_layers.3.output.LayerNorm.weight', 'bert.encoder.word_layers.3.output.LayerNorm.bias', 'bert.encoder.word_layers.4.attention.self.query.weight', 'bert.encoder.word_layers.4.attention.self.query.bias', 'bert.encoder.word_layers.4.attention.self.key.weight', 'bert.encoder.word_layers.4.attention.self.key.bias', 'bert.encoder.word_layers.4.attention.self.value.weight', 'bert.encoder.word_layers.4.attention.self.value.bias', 'bert.encoder.word_layers.4.attention.output.dense.weight', 'bert.encoder.word_layers.4.attention.output.dense.bias', 'bert.encoder.word_layers.4.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.4.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.4.intermediate.dense.weight', 'bert.encoder.word_layers.4.intermediate.dense.bias', 'bert.encoder.word_layers.4.output.dense.weight', 'bert.encoder.word_layers.4.output.dense.bias', 'bert.encoder.word_layers.4.output.LayerNorm.weight', 'bert.encoder.word_layers.4.output.LayerNorm.bias', 'bert.encoder.word_layers.5.attention.self.query.weight', 'bert.encoder.word_layers.5.attention.self.query.bias', 'bert.encoder.word_layers.5.attention.self.key.weight', 'bert.encoder.word_layers.5.attention.self.key.bias', 'bert.encoder.word_layers.5.attention.self.value.weight', 'bert.encoder.word_layers.5.attention.self.value.bias', 'bert.encoder.word_layers.5.attention.output.dense.weight', 'bert.encoder.word_layers.5.attention.output.dense.bias', 'bert.encoder.word_layers.5.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.5.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.5.intermediate.dense.weight', 'bert.encoder.word_layers.5.intermediate.dense.bias', 'bert.encoder.word_layers.5.output.dense.weight', 'bert.encoder.word_layers.5.output.dense.bias', 'bert.encoder.word_layers.5.output.LayerNorm.weight', 'bert.encoder.word_layers.5.output.LayerNorm.bias']
09/11/2023 17:17:06 - INFO - __main__ -   # of trainable parameters: 103133952
09/11/2023 17:17:07 - INFO - __main__ -   ***** Running training *****
09/11/2023 17:17:07 - INFO - __main__ -     Num examples = 63
09/11/2023 17:17:07 - INFO - __main__ -     Batch size = 16
09/11/2023 17:17:07 - INFO - __main__ -     Num steps = 150
09/11/2023 17:17:10 - INFO - __main__ -   OOV: 0.029056
09/11/2023 17:17:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:10 - INFO - __main__ -   
Epoch: 1, P: 0.146112, R: 0.133319, F: 0.139423, OOV: 0.029056
09/11/2023 17:17:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:14 - INFO - __main__ -   OOV: 0.034167
09/11/2023 17:17:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:14 - INFO - __main__ -   
Epoch: 2, P: 0.392438, R: 0.208016, F: 0.271906, OOV: 0.034167
09/11/2023 17:17:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:19 - INFO - __main__ -   OOV: 0.290557
09/11/2023 17:17:19 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:19 - INFO - __main__ -   
Epoch: 3, P: 0.490904, R: 0.546565, F: 0.517241, OOV: 0.290557
09/11/2023 17:17:19 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:23 - INFO - __main__ -   OOV: 0.598063
09/11/2023 17:17:23 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:23 - INFO - __main__ -   
Epoch: 4, P: 0.799046, R: 0.754260, F: 0.776007, OOV: 0.598063
09/11/2023 17:17:23 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:28 - INFO - __main__ -   OOV: 0.700834
09/11/2023 17:17:28 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:28 - INFO - __main__ -   
Epoch: 5, P: 0.837755, R: 0.822313, F: 0.829962, OOV: 0.700834
09/11/2023 17:17:28 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:32 - INFO - __main__ -   OOV: 0.722895
09/11/2023 17:17:32 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:32 - INFO - __main__ -   
Epoch: 6, P: 0.834973, R: 0.855107, F: 0.844920, OOV: 0.722895
09/11/2023 17:17:32 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:36 - INFO - __main__ -   OOV: 0.740920
09/11/2023 17:17:36 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:36 - INFO - __main__ -   
Epoch: 7, P: 0.884226, R: 0.851249, F: 0.867424, OOV: 0.740920
09/11/2023 17:17:36 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:41 - INFO - __main__ -   OOV: 0.776702
09/11/2023 17:17:41 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:41 - INFO - __main__ -   
Epoch: 8, P: 0.875358, R: 0.884364, F: 0.879838, OOV: 0.776702
09/11/2023 17:17:41 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:45 - INFO - __main__ -   OOV: 0.780199
09/11/2023 17:17:45 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:45 - INFO - __main__ -   
Epoch: 9, P: 0.899017, R: 0.882542, F: 0.890704, OOV: 0.780199
09/11/2023 17:17:45 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:49 - INFO - __main__ -   OOV: 0.775087
09/11/2023 17:17:49 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:49 - INFO - __main__ -   
Epoch: 10, P: 0.908624, R: 0.872790, F: 0.890347, OOV: 0.775087
09/11/2023 17:17:49 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:53 - INFO - __main__ -   OOV: 0.791499
09/11/2023 17:17:53 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:53 - INFO - __main__ -   
Epoch: 11, P: 0.900773, R: 0.886293, F: 0.893475, OOV: 0.791499
09/11/2023 17:17:53 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:57 - INFO - __main__ -   OOV: 0.796072
09/11/2023 17:17:57 - INFO - __main__ -   =======entity level========
09/11/2023 17:17:57 - INFO - __main__ -   
Epoch: 12, P: 0.895285, R: 0.893366, F: 0.894325, OOV: 0.796072
09/11/2023 17:17:57 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:02 - INFO - __main__ -   OOV: 0.798493
09/11/2023 17:18:02 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:02 - INFO - __main__ -   
Epoch: 13, P: 0.905197, R: 0.892295, F: 0.898699, OOV: 0.798493
09/11/2023 17:18:02 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:06 - INFO - __main__ -   OOV: 0.799301
09/11/2023 17:18:06 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:06 - INFO - __main__ -   
Epoch: 14, P: 0.910605, R: 0.890794, F: 0.900590, OOV: 0.799301
09/11/2023 17:18:06 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:10 - INFO - __main__ -   OOV: 0.796879
09/11/2023 17:18:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:10 - INFO - __main__ -   
Epoch: 15, P: 0.911487, R: 0.889508, F: 0.900363, OOV: 0.796879
09/11/2023 17:18:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:14 - INFO - __main__ -   OOV: 0.799301
09/11/2023 17:18:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:14 - INFO - __main__ -   
Epoch: 16, P: 0.914727, R: 0.888651, F: 0.901500, OOV: 0.799301
09/11/2023 17:18:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:18 - INFO - __main__ -   OOV: 0.800646
09/11/2023 17:18:18 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:18 - INFO - __main__ -   
Epoch: 17, P: 0.914270, R: 0.889187, F: 0.901554, OOV: 0.800646
09/11/2023 17:18:18 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:22 - INFO - __main__ -   OOV: 0.801453
09/11/2023 17:18:22 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:22 - INFO - __main__ -   
Epoch: 18, P: 0.912466, R: 0.890365, F: 0.901280, OOV: 0.801453
09/11/2023 17:18:22 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:26 - INFO - __main__ -   OOV: 0.802798
09/11/2023 17:18:26 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:26 - INFO - __main__ -   
Epoch: 19, P: 0.912190, R: 0.891759, F: 0.901859, OOV: 0.802798
09/11/2023 17:18:26 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:30 - INFO - __main__ -   OOV: 0.802798
09/11/2023 17:18:30 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:30 - INFO - __main__ -   
Epoch: 20, P: 0.910941, R: 0.892295, F: 0.901521, OOV: 0.802798
09/11/2023 17:18:30 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:34 - INFO - __main__ -   OOV: 0.802260
09/11/2023 17:18:34 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:34 - INFO - __main__ -   
Epoch: 21, P: 0.910352, R: 0.893473, F: 0.901834, OOV: 0.802260
09/11/2023 17:18:34 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:38 - INFO - __main__ -   OOV: 0.802798
09/11/2023 17:18:38 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:38 - INFO - __main__ -   
Epoch: 22, P: 0.909953, R: 0.894545, F: 0.902183, OOV: 0.802798
09/11/2023 17:18:38 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:42 - INFO - __main__ -   OOV: 0.801722
09/11/2023 17:18:42 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:42 - INFO - __main__ -   
Epoch: 23, P: 0.909468, R: 0.893581, F: 0.901454, OOV: 0.801722
09/11/2023 17:18:42 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:45 - INFO - __main__ -   OOV: 0.801453
09/11/2023 17:18:45 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:45 - INFO - __main__ -   
Epoch: 24, P: 0.909369, R: 0.893581, F: 0.901405, OOV: 0.801453
09/11/2023 17:18:45 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:49 - INFO - __main__ -   OOV: 0.801722
09/11/2023 17:18:49 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:49 - INFO - __main__ -   
Epoch: 25, P: 0.906967, R: 0.894331, F: 0.900604, OOV: 0.801722
09/11/2023 17:18:49 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:52 - INFO - __main__ -   OOV: 0.801991
09/11/2023 17:18:52 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:52 - INFO - __main__ -   
Epoch: 26, P: 0.904169, R: 0.894867, F: 0.899494, OOV: 0.801991
09/11/2023 17:18:52 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:56 - INFO - __main__ -   OOV: 0.801453
09/11/2023 17:18:56 - INFO - __main__ -   =======entity level========
09/11/2023 17:18:56 - INFO - __main__ -   
Epoch: 27, P: 0.901544, R: 0.894974, F: 0.898247, OOV: 0.801453
09/11/2023 17:18:56 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:00 - INFO - __main__ -   OOV: 0.801722
09/11/2023 17:19:00 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:00 - INFO - __main__ -   
Epoch: 28, P: 0.901079, R: 0.895188, F: 0.898124, OOV: 0.801722
09/11/2023 17:19:00 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:03 - INFO - __main__ -   OOV: 0.802260
09/11/2023 17:19:03 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:03 - INFO - __main__ -   
Epoch: 29, P: 0.902376, R: 0.895510, F: 0.898930, OOV: 0.802260
09/11/2023 17:19:03 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:07 - INFO - __main__ -   OOV: 0.802260
09/11/2023 17:19:07 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:07 - INFO - __main__ -   
Epoch: 30, P: 0.904494, R: 0.895188, F: 0.899817, OOV: 0.802260
09/11/2023 17:19:07 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:10 - INFO - __main__ -   OOV: 0.800646
09/11/2023 17:19:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:10 - INFO - __main__ -   
Epoch: 31, P: 0.905677, R: 0.894224, F: 0.899914, OOV: 0.800646
09/11/2023 17:19:10 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:14 - INFO - __main__ -   OOV: 0.801184
09/11/2023 17:19:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:14 - INFO - __main__ -   
Epoch: 32, P: 0.907706, R: 0.893795, F: 0.900697, OOV: 0.801184
09/11/2023 17:19:14 - INFO - __main__ -   =======entity level========
09/11/2023 17:19:14 - INFO - __main__ -   
Early stop triggered at epoch 31

09/11/2023 17:19:14 - INFO - __main__ -   
=======best f entity level========
09/11/2023 17:19:14 - INFO - __main__ -   
Epoch: 22, P: 0.909953, R: 0.894545, F: 0.902183, OOV: 0.802798

09/11/2023 17:19:14 - INFO - __main__ -   
=======best f entity level========
