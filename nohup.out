{'do_train': True, 'do_test': False, 'do_predict': False, 'do_predict_uc': False, 'train_data_path': './data/mydata/larger/train.tsv', 'self_train_data_path': None, 'eval_data_path': './data/mydata/larger/dev.tsv', 'input_file': None, 'output_file': None, 'output_file_tsv': None, 'use_bert': True, 'use_zen': False, 'bert_model': '/home/testcross/projects/models/bert-base-chinese', 'eval_model': None, 'cache_dir': '', 'max_seq_length': 300, 'max_ngram_size': 300, 'do_lower_case': False, 'train_batch_size': 16, 'eval_batch_size': 16, 'lambda_rate': 0.5, 'c_th': 0.7, 'u_th': 0.8, 'learning_rate': 1e-05, 'num_train_epochs': 50.0, 'warmup_proportion': 0.1, 'no_cuda': False, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'fp16': False, 'loss_scale': 0, 'server_ip': '', 'server_port': '', 'patient': 10, 'ngram_num_threshold': 3, 'av_threshold': 1, 'max_ngram_length': 5, 'model_name': 'bert_wm_av_3_score_0.7_0.8', 'use_memory': True, 'use_cnn': False, 'decoder': 'softmax', 'ngram_flag': 'av', 'save_top': 1}
device: cuda n_gpu: 2, distributed training: False, 16-bits training: False
# of word in train: 1079: 
# of n-gram in memory: 4056
Traceback (most recent call last):
  File "my_wmseg_main.py", line 1447, in <module>
    main()
  File "my_wmseg_main.py", line 1435, in main
    train(args)
  File "my_wmseg_main.py", line 104, in train
    seg_model = WMSeg(word2id, gram2id, label_map, hpara, args)
  File "/home/testcross/projects/WMSeg/my_wmseg_model.py", line 173, in __init__
    self.bert = BertModel.from_pretrained(args.bert_model, cache_dir=cache_dir)
  File "/home/testcross/projects/WMSeg/pytorch_pretrained_bert/modeling.py", line 718, in from_pretrained
    model = cls(config, *inputs, **kwargs)
  File "/home/testcross/projects/WMSeg/pytorch_pretrained_bert/modeling.py", line 824, in __init__
    self.embeddings = BertEmbeddings(config)
  File "/home/testcross/projects/WMSeg/pytorch_pretrained_bert/modeling.py", line 299, in __init__
    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
  File "/home/testcross/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 100, in __init__
    self.reset_parameters()
  File "/home/testcross/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 108, in reset_parameters
    init.normal_(self.weight)
  File "/home/testcross/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/init.py", line 108, in normal_
    return _no_grad_normal_(tensor, mean, std)
  File "/home/testcross/anaconda3/envs/master/lib/python3.6/site-packages/torch/nn/init.py", line 20, in _no_grad_normal_
    return tensor.normal_(mean, std)
KeyboardInterrupt
