05/29/2023 18:52:37 - INFO - pytorch_pretrained_zen.modeling -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
05/29/2023 18:52:37 - INFO - __main__ -   {'do_train': True, 'do_test': False, 'do_predict': False, 'train_data_path': './data/mydata/geo/my/train.tsv', 'eval_data_path': './data/mydata/geo/my/test.tsv', 'input_file': None, 'output_file': None, 'use_bert': True, 'use_zen': False, 'bert_model': '/opt/Projects/Python/ZEN/models/GeoZen/TEST_Model', 'eval_model': None, 'cache_dir': '', 'max_seq_length': 300, 'max_ngram_size': 300, 'do_lower_case': False, 'train_batch_size': 16, 'eval_batch_size': 16, 'learning_rate': 1e-05, 'num_train_epochs': 50.0, 'warmup_proportion': 0.1, 'no_cuda': False, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'fp16': False, 'loss_scale': 0, 'server_ip': '', 'server_port': '', 'patient': 10, 'ngram_num_threshold': 2, 'av_threshold': 3, 'max_ngram_length': 5, 'model_name': 'geo_zen_memory_crf', 'use_memory': True, 'decoder': 'crf', 'ngram_flag': 'av', 'save_top': 1}
05/29/2023 18:52:37 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
05/29/2023 18:52:37 - INFO - __main__ -   # of word in train: 378: 
05/29/2023 18:52:37 - INFO - __main__ -   # of n-gram in memory: 205
05/29/2023 18:52:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /opt/Projects/Python/ZEN/models/GeoZen/TEST_Model/vocab.txt
05/29/2023 18:52:37 - INFO - pytorch_pretrained_bert.modeling -   loading weights file /opt/Projects/Python/ZEN/models/GeoZen/TEST_Model/pytorch_model.bin
05/29/2023 18:52:37 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file /opt/Projects/Python/ZEN/models/GeoZen/TEST_Model/config.json
05/29/2023 18:52:37 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_hidden_word_layers": 6,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128,
  "word_size": 104089
}

05/29/2023 18:52:38 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertModel: ['bert.word_embeddings.word_embeddings.weight', 'bert.word_embeddings.token_type_embeddings.weight', 'bert.word_embeddings.LayerNorm.weight', 'bert.word_embeddings.LayerNorm.bias', 'bert.encoder.word_layers.0.attention.self.query.weight', 'bert.encoder.word_layers.0.attention.self.query.bias', 'bert.encoder.word_layers.0.attention.self.key.weight', 'bert.encoder.word_layers.0.attention.self.key.bias', 'bert.encoder.word_layers.0.attention.self.value.weight', 'bert.encoder.word_layers.0.attention.self.value.bias', 'bert.encoder.word_layers.0.attention.output.dense.weight', 'bert.encoder.word_layers.0.attention.output.dense.bias', 'bert.encoder.word_layers.0.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.0.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.0.intermediate.dense.weight', 'bert.encoder.word_layers.0.intermediate.dense.bias', 'bert.encoder.word_layers.0.output.dense.weight', 'bert.encoder.word_layers.0.output.dense.bias', 'bert.encoder.word_layers.0.output.LayerNorm.weight', 'bert.encoder.word_layers.0.output.LayerNorm.bias', 'bert.encoder.word_layers.1.attention.self.query.weight', 'bert.encoder.word_layers.1.attention.self.query.bias', 'bert.encoder.word_layers.1.attention.self.key.weight', 'bert.encoder.word_layers.1.attention.self.key.bias', 'bert.encoder.word_layers.1.attention.self.value.weight', 'bert.encoder.word_layers.1.attention.self.value.bias', 'bert.encoder.word_layers.1.attention.output.dense.weight', 'bert.encoder.word_layers.1.attention.output.dense.bias', 'bert.encoder.word_layers.1.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.1.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.1.intermediate.dense.weight', 'bert.encoder.word_layers.1.intermediate.dense.bias', 'bert.encoder.word_layers.1.output.dense.weight', 'bert.encoder.word_layers.1.output.dense.bias', 'bert.encoder.word_layers.1.output.LayerNorm.weight', 'bert.encoder.word_layers.1.output.LayerNorm.bias', 'bert.encoder.word_layers.2.attention.self.query.weight', 'bert.encoder.word_layers.2.attention.self.query.bias', 'bert.encoder.word_layers.2.attention.self.key.weight', 'bert.encoder.word_layers.2.attention.self.key.bias', 'bert.encoder.word_layers.2.attention.self.value.weight', 'bert.encoder.word_layers.2.attention.self.value.bias', 'bert.encoder.word_layers.2.attention.output.dense.weight', 'bert.encoder.word_layers.2.attention.output.dense.bias', 'bert.encoder.word_layers.2.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.2.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.2.intermediate.dense.weight', 'bert.encoder.word_layers.2.intermediate.dense.bias', 'bert.encoder.word_layers.2.output.dense.weight', 'bert.encoder.word_layers.2.output.dense.bias', 'bert.encoder.word_layers.2.output.LayerNorm.weight', 'bert.encoder.word_layers.2.output.LayerNorm.bias', 'bert.encoder.word_layers.3.attention.self.query.weight', 'bert.encoder.word_layers.3.attention.self.query.bias', 'bert.encoder.word_layers.3.attention.self.key.weight', 'bert.encoder.word_layers.3.attention.self.key.bias', 'bert.encoder.word_layers.3.attention.self.value.weight', 'bert.encoder.word_layers.3.attention.self.value.bias', 'bert.encoder.word_layers.3.attention.output.dense.weight', 'bert.encoder.word_layers.3.attention.output.dense.bias', 'bert.encoder.word_layers.3.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.3.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.3.intermediate.dense.weight', 'bert.encoder.word_layers.3.intermediate.dense.bias', 'bert.encoder.word_layers.3.output.dense.weight', 'bert.encoder.word_layers.3.output.dense.bias', 'bert.encoder.word_layers.3.output.LayerNorm.weight', 'bert.encoder.word_layers.3.output.LayerNorm.bias', 'bert.encoder.word_layers.4.attention.self.query.weight', 'bert.encoder.word_layers.4.attention.self.query.bias', 'bert.encoder.word_layers.4.attention.self.key.weight', 'bert.encoder.word_layers.4.attention.self.key.bias', 'bert.encoder.word_layers.4.attention.self.value.weight', 'bert.encoder.word_layers.4.attention.self.value.bias', 'bert.encoder.word_layers.4.attention.output.dense.weight', 'bert.encoder.word_layers.4.attention.output.dense.bias', 'bert.encoder.word_layers.4.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.4.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.4.intermediate.dense.weight', 'bert.encoder.word_layers.4.intermediate.dense.bias', 'bert.encoder.word_layers.4.output.dense.weight', 'bert.encoder.word_layers.4.output.dense.bias', 'bert.encoder.word_layers.4.output.LayerNorm.weight', 'bert.encoder.word_layers.4.output.LayerNorm.bias', 'bert.encoder.word_layers.5.attention.self.query.weight', 'bert.encoder.word_layers.5.attention.self.query.bias', 'bert.encoder.word_layers.5.attention.self.key.weight', 'bert.encoder.word_layers.5.attention.self.key.bias', 'bert.encoder.word_layers.5.attention.self.value.weight', 'bert.encoder.word_layers.5.attention.self.value.bias', 'bert.encoder.word_layers.5.attention.output.dense.weight', 'bert.encoder.word_layers.5.attention.output.dense.bias', 'bert.encoder.word_layers.5.attention.output.LayerNorm.weight', 'bert.encoder.word_layers.5.attention.output.LayerNorm.bias', 'bert.encoder.word_layers.5.intermediate.dense.weight', 'bert.encoder.word_layers.5.intermediate.dense.bias', 'bert.encoder.word_layers.5.output.dense.weight', 'bert.encoder.word_layers.5.output.dense.bias', 'bert.encoder.word_layers.5.output.LayerNorm.weight', 'bert.encoder.word_layers.5.output.LayerNorm.bias']
05/29/2023 18:52:39 - INFO - __main__ -   # of trainable parameters: 102438976
05/29/2023 18:52:39 - INFO - __main__ -   ***** Running training *****
05/29/2023 18:52:39 - INFO - __main__ -     Num examples = 41
05/29/2023 18:52:39 - INFO - __main__ -     Batch size = 16
05/29/2023 18:52:39 - INFO - __main__ -     Num steps = 100
05/29/2023 18:52:40 - INFO - __main__ -   OOV: 0.027397
05/29/2023 18:52:40 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:40 - INFO - __main__ -   
Epoch: 1, P: 0.058559, R: 0.025515, F: 0.035543, OOV: 0.027397
05/29/2023 18:52:40 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:42 - INFO - __main__ -   OOV: 0.098630
05/29/2023 18:52:42 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:42 - INFO - __main__ -   
Epoch: 2, P: 0.269231, R: 0.261040, F: 0.265072, OOV: 0.098630
05/29/2023 18:52:42 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:43 - INFO - __main__ -   OOV: 0.197260
05/29/2023 18:52:43 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:43 - INFO - __main__ -   
Epoch: 3, P: 0.440966, R: 0.483808, F: 0.461394, OOV: 0.197260
05/29/2023 18:52:43 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:45 - INFO - __main__ -   OOV: 0.397260
05/29/2023 18:52:45 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:45 - INFO - __main__ -   
Epoch: 4, P: 0.702259, R: 0.671246, F: 0.686402, OOV: 0.397260
05/29/2023 18:52:45 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:47 - INFO - __main__ -   OOV: 0.572603
05/29/2023 18:52:47 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:47 - INFO - __main__ -   
Epoch: 5, P: 0.790514, R: 0.785083, F: 0.787789, OOV: 0.572603
05/29/2023 18:52:47 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:49 - INFO - __main__ -   OOV: 0.665753
05/29/2023 18:52:49 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:49 - INFO - __main__ -   
Epoch: 6, P: 0.860226, R: 0.821394, F: 0.840361, OOV: 0.665753
05/29/2023 18:52:49 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:51 - INFO - __main__ -   OOV: 0.731507
05/29/2023 18:52:51 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:51 - INFO - __main__ -   
Epoch: 7, P: 0.871769, R: 0.860648, F: 0.866173, OOV: 0.731507
05/29/2023 18:52:51 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:53 - INFO - __main__ -   OOV: 0.772603
05/29/2023 18:52:53 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:53 - INFO - __main__ -   
Epoch: 8, P: 0.876471, R: 0.877331, F: 0.876900, OOV: 0.772603
05/29/2023 18:52:53 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:55 - INFO - __main__ -   OOV: 0.789041
05/29/2023 18:52:55 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:55 - INFO - __main__ -   
Epoch: 9, P: 0.899800, R: 0.881256, F: 0.890431, OOV: 0.789041
05/29/2023 18:52:55 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:56 - INFO - __main__ -   OOV: 0.794521
05/29/2023 18:52:56 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:56 - INFO - __main__ -   
Epoch: 10, P: 0.904234, R: 0.880275, F: 0.892093, OOV: 0.794521
05/29/2023 18:52:56 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:58 - INFO - __main__ -   OOV: 0.780822
05/29/2023 18:52:58 - INFO - __main__ -   =======entity level========
05/29/2023 18:52:58 - INFO - __main__ -   
Epoch: 11, P: 0.890656, R: 0.879293, F: 0.884938, OOV: 0.780822
05/29/2023 18:52:58 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:00 - INFO - __main__ -   OOV: 0.775342
05/29/2023 18:53:00 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:00 - INFO - __main__ -   
Epoch: 12, P: 0.893000, R: 0.876349, F: 0.884596, OOV: 0.775342
05/29/2023 18:53:00 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:01 - INFO - __main__ -   OOV: 0.769863
05/29/2023 18:53:01 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:01 - INFO - __main__ -   
Epoch: 13, P: 0.890220, R: 0.875368, F: 0.882731, OOV: 0.769863
05/29/2023 18:53:01 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:02 - INFO - __main__ -   OOV: 0.775342
05/29/2023 18:53:02 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:02 - INFO - __main__ -   
Epoch: 14, P: 0.891109, R: 0.875368, F: 0.883168, OOV: 0.775342
05/29/2023 18:53:02 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:03 - INFO - __main__ -   OOV: 0.778082
05/29/2023 18:53:03 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:03 - INFO - __main__ -   
Epoch: 15, P: 0.891434, R: 0.878312, F: 0.884825, OOV: 0.778082
05/29/2023 18:53:03 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:05 - INFO - __main__ -   OOV: 0.791781
05/29/2023 18:53:05 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:05 - INFO - __main__ -   
Epoch: 16, P: 0.889437, R: 0.884200, F: 0.886811, OOV: 0.791781
05/29/2023 18:53:05 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:06 - INFO - __main__ -   OOV: 0.791781
05/29/2023 18:53:06 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:06 - INFO - __main__ -   
Epoch: 17, P: 0.885406, R: 0.887144, F: 0.886275, OOV: 0.791781
05/29/2023 18:53:06 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:07 - INFO - __main__ -   OOV: 0.786301
05/29/2023 18:53:07 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:07 - INFO - __main__ -   
Epoch: 18, P: 0.884086, R: 0.883219, F: 0.883652, OOV: 0.786301
05/29/2023 18:53:07 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:08 - INFO - __main__ -   OOV: 0.794521
05/29/2023 18:53:08 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:08 - INFO - __main__ -   
Epoch: 19, P: 0.894632, R: 0.883219, F: 0.888889, OOV: 0.794521
05/29/2023 18:53:08 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:10 - INFO - __main__ -   OOV: 0.794521
05/29/2023 18:53:10 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:10 - INFO - __main__ -   
Epoch: 20, P: 0.890438, R: 0.877331, F: 0.883836, OOV: 0.794521
05/29/2023 18:53:10 - INFO - __main__ -   =======entity level========
05/29/2023 18:53:10 - INFO - __main__ -   
Early stop triggered at epoch 19

05/29/2023 18:53:10 - INFO - __main__ -   
=======best f entity level========
05/29/2023 18:53:10 - INFO - __main__ -   
Epoch: 10, P: 0.904234, R: 0.880275, F: 0.892093, OOV: 0.794521

05/29/2023 18:53:10 - INFO - __main__ -   
=======best f entity level========
